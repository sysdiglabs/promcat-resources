---
apiVersion: v1
kind: Alert
app: "Kubernetes control plane"
version: 1.0.0
appVersion: 
  - 1.14.0
maintainers:
  - name: Sysdig team
    link: https://sysdig.com/
description: |
  # Alerts
  ## KubePodCrashLooping
  The pod can become in crashloopback error.
  This alert trigger when the pod is having a lot of error and it become in a loopbackoff

  ## KubeCPUOvercommit

  ## KubeMemOvercommit

  ## KubeQuotaExceeded

  ## CPUThrottlingHigh

  ## KubePersistentVolumeUsageCritical

  ## KubePersistentVolumeFullInFourDays

  ## KubePersistentVolumeErrors

  ## KubeVersionMismatch

  ## KubeClientErrors

  ## ErrorBudgetBurn

  ## KubeAPILatencyHigh

  ## KubeAPIErrorsHigh

  ## KubeClientCertificateExpiration

  ## AggregatedAPIErrors

  ## AggregatedAPIDown

  ## KubeAPIDown

  ## KubeNodeNotReady

  ## KubeNodeUnreachable

  ## KubeletTooManyPods

  ## KubeNodeReadinessFlapping

  ## KubeletPlegDurationHigh

  ## KubeletPodStartUpLatencyHigh

  ## KubeletDown

  ## KubeSchedulerDown

  ## KubeControllerManagerDown

  # Installing the alerts
  ## In Prometheus
  Copy the alerts for Prometheus and paste them inside of the configuration file in the groups section:
  ```yaml
  [...]
  groups:
  - name: example
      # Insert here the alerts
      rules: 
  ``` 
  ## In Sysdig Monitor
  To install these alerts in Sysdig Monitor, contact with the support service. 
configurations: 
  - kind: Prometheus
    data: |
      - "alert": "KubeCPUOvercommit"
        "annotations":
          "message": "Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit"
        "expr": |
          sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum{})
            /
          sum(kube_node_status_allocatable_cpu_cores)
            >
          (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
        "for": "5m"
        "labels":
          "severity": "warning"
      - "alert": "KubeMemOvercommit"
        "annotations":
          "message": "Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit"
        "expr": |
          sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{})
            /
          sum(kube_node_status_allocatable_memory_bytes)
            >
          (count(kube_node_status_allocatable_memory_bytes)-1)
            /
          count(kube_node_status_allocatable_memory_bytes)
        "for": "5m"
        "labels":
          "severity": "warning"
      - "alert": "KubeCPUOvercommit"
        "annotations":
          "message": "Cluster has overcommitted CPU resource requests for Namespaces."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit"
        "expr": |
          sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
            /
          sum(kube_node_status_allocatable_cpu_cores)
            > 1.5
        "for": "5m"
        "labels":
          "severity": "warning"
      - "alert": "KubeMemOvercommit"
        "annotations":
          "message": "Cluster has overcommitted memory resource requests for Namespaces."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit"
        "expr": |
          sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
            /
          sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"})
            > 1.5
        "for": "5m"
        "labels":
          "severity": "warning"
      - "alert": "KubeQuotaExceeded"
        "annotations":
          "message": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded"
        "expr": |
          kube_resourcequota{job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
            > 0.90
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "CPUThrottlingHigh"
        "annotations":
          "message": "{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh"
        "expr": |
          sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (container, pod, namespace)
            /
          sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)
            > ( 25 / 100 )
        "for": "15m"
        "labels":
          "severity": "warning"
    
      - "alert": "KubePersistentVolumeUsageCritical"
        "annotations":
          "message": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical"
        "expr": |
          kubelet_volume_stats_available_bytes{job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes{job="kubelet"}
            < 0.03
        "for": "1m"
        "labels":
          "severity": "critical"
      - "alert": "KubePersistentVolumeFullInFourDays"
        "annotations":
          "message": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays"
        "expr": |
          (
            kubelet_volume_stats_available_bytes{job="kubelet"}
              /
            kubelet_volume_stats_capacity_bytes{job="kubelet"}
          ) < 0.15
          and
          predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
        "for": "1h"
        "labels":
          "severity": "critical"
      - "alert": "KubePersistentVolumeErrors"
        "annotations":
          "message": "The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors"
        "expr": |
          kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
        "for": "5m"
        "labels":
          "severity": "critical"
    
      - "alert": "KubeVersionMismatch"
        "annotations":
          "message": "There are {{ $value }} different semantic versions of Kubernetes components running."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch"
        "expr": |
          count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "KubeClientErrors"
        "annotations":
          "message": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'"
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors"
        "expr": |
          (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
            /
          sum(rate(rest_client_requests_total[5m])) by (instance, job))
          > 0.01
        "for": "15m"
        "labels":
          "severity": "warning"
    
      - "alert": "ErrorBudgetBurn"
        "annotations":
          "message": "High requests error budget burn for job=kube-apiserver (current value: {{ $value }})"
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn"
        "expr": |
          (
            status_class_5xx:apiserver_request_total:ratio_rate1h{job="kube-apiserver"} > (14.4*0.010000)
            and
            status_class_5xx:apiserver_request_total:ratio_rate5m{job="kube-apiserver"} > (14.4*0.010000)
          )
          or
          (
            status_class_5xx:apiserver_request_total:ratio_rate6h{job="kube-apiserver"} > (6*0.010000)
            and
            status_class_5xx:apiserver_request_total:ratio_rate30m{job="kube-apiserver"} > (6*0.010000)
          )
        "labels":
          "job": "kube-apiserver"
          "severity": "critical"
      - "alert": "ErrorBudgetBurn"
        "annotations":
          "message": "High requests error budget burn for job=kube-apiserver (current value: {{ $value }})"
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn"
        "expr": |
          (
            status_class_5xx:apiserver_request_total:ratio_rate1d{job="kube-apiserver"} > (3*0.010000)
            and
            status_class_5xx:apiserver_request_total:ratio_rate2h{job="kube-apiserver"} > (3*0.010000)
          )
          or
          (
            status_class_5xx:apiserver_request_total:ratio_rate3d{job="kube-apiserver"} > (0.010000)
            and
            status_class_5xx:apiserver_request_total:ratio_rate6h{job="kube-apiserver"} > (0.010000)
          )
        "labels":
          "job": "kube-apiserver"
          "severity": "warning"
    
      - "alert": "KubeAPILatencyHigh"
        "annotations":
          "message": "The API server has an abnormal latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh"
        "expr": |
          (
            cluster:apiserver_request_duration_seconds:mean5m{job="kube-apiserver"}
            >
            on (verb) group_left()
            (
              avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kube-apiserver"} >= 0)
              +
              2*stddev by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kube-apiserver"} >= 0)
            )
          ) > on (verb) group_left()
          1.2 * avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kube-apiserver"} >= 0)
          and on (verb,resource)
          cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="kube-apiserver",quantile="0.99"}
          >
          1
        "for": "5m"
        "labels":
          "severity": "warning"
      - "alert": "KubeAPILatencyHigh"
        "annotations":
          "message": "The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh"
        "expr": |
          cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="kube-apiserver",quantile="0.99"} > 4
        "for": "10m"
        "labels":
          "severity": "critical"
      - "alert": "KubeAPIErrorsHigh"
        "annotations":
          "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh"
        "expr": |
          sum(rate(apiserver_request_total{job="kube-apiserver",code=~"5.."}[5m])) by (resource,subresource,verb)
            /
          sum(rate(apiserver_request_total{job="kube-apiserver"}[5m])) by (resource,subresource,verb) > 0.10
        "for": "10m"
        "labels":
          "severity": "critical"
      - "alert": "KubeAPIErrorsHigh"
        "annotations":
          "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh"
        "expr": |
          sum(rate(apiserver_request_total{job="kube-apiserver",code=~"5.."}[5m])) by (resource,subresource,verb)
            /
          sum(rate(apiserver_request_total{job="kube-apiserver"}[5m])) by (resource,subresource,verb) > 0.05
        "for": "10m"
        "labels":
          "severity": "warning"
      - "alert": "KubeClientCertificateExpiration"
        "annotations":
          "message": "A client certificate used to authenticate to the apiserver is expiring in less than 7.0 days."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration"
        "expr": |
          apiserver_client_certificate_expiration_seconds_count{job="kube-apiserver"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kube-apiserver"}[5m]))) < 604800
        "labels":
          "severity": "warning"
      - "alert": "KubeClientCertificateExpiration"
        "annotations":
          "message": "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration"
        "expr": |
          apiserver_client_certificate_expiration_seconds_count{job="kube-apiserver"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="kube-apiserver"}[5m]))) < 86400
        "labels":
          "severity": "critical"
      - "alert": "AggregatedAPIErrors"
        "annotations":
          "message": "An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. The number of errors have increased for it in the past five minutes. High values indicate that the availability of the service changes too often."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapierrors"
        "expr": |
          sum by(name, namespace)(increase(aggregator_unavailable_apiservice_count[5m])) > 2
        "labels":
          "severity": "warning"
      - "alert": "AggregatedAPIDown"
        "annotations":
          "message": "An aggregated API {{ $labels.name }}/{{ $labels.namespace }} is down. It has not been available at least for the past five minutes."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapidown"
        "expr": |
          sum by(name, namespace)(sum_over_time(aggregator_unavailable_apiservice[5m])) > 0
        "for": "5m"
        "labels":
          "severity": "warning"
      - "alert": "KubeAPIDown"
        "annotations":
          "message": "KubeAPI has disappeared from Prometheus target discovery."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown"
        "expr": |
          absent(up{job="kube-apiserver"} == 1)
        "for": "15m"
        "labels":
          "severity": "critical"
    
      - "alert": "KubeNodeNotReady"
        "annotations":
          "message": "{{ $labels.node }} has been unready for more than 15 minutes."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready"
        "expr": |
          kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "KubeNodeUnreachable"
        "annotations":
          "message": "{{ $labels.node }} is unreachable and some workloads may be rescheduled."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodeunreachable"
        "expr": |
          kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} == 1
        "for": "2m"
        "labels":
          "severity": "warning"
      - "alert": "KubeletTooManyPods"
        "annotations":
          "message": "Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods"
        "expr": |
          max(max(kubelet_running_pod_count{job="kubelet"}) by(instance) * on(instance) group_left(node) kubelet_node_name{job="kubelet"}) by(node) / max(kube_node_status_capacity_pods{job="kube-state-metrics"}) by(node) > 0.95
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "KubeNodeReadinessFlapping"
        "annotations":
          "message": "The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodereadinessflapping"
        "expr": |
          sum(changes(kube_node_status_condition{status="true",condition="Ready"}[15m])) by (node) > 2
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "KubeletPlegDurationHigh"
        "annotations":
          "message": "The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletplegdurationhigh"
        "expr": |
          node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
        "for": "5m"
        "labels":
          "severity": "warning"
      - "alert": "KubeletPodStartUpLatencyHigh"
        "annotations":
          "message": "Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletpodstartuplatencyhigh"
        "expr": |
          histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet"}[5m])) by (instance, le)) * on(instance) group_left(node) kubelet_node_name  > 60
        "for": "15m"
        "labels":
          "severity": "warning"
      - "alert": "KubeletDown"
        "annotations":
          "message": "Kubelet has disappeared from Prometheus target discovery."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown"
        "expr": |
          absent(up{job="kubelet"} == 1)
        "for": "15m"
        "labels":
          "severity": "critical"
    
      - "alert": "KubeSchedulerDown"
        "annotations":
          "message": "KubeScheduler has disappeared from Prometheus target discovery."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown"
        "expr": |
          absent(up{job="kube-scheduler"} == 1)
        "for": "15m"
        "labels":
          "severity": "critical"
    
      - "alert": "KubeControllerManagerDown"
        "annotations":
          "message": "KubeControllerManager has disappeared from Prometheus target discovery."
          "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown"
        "expr": |
          absent(up{job="kube-controller-manager"} == 1)
        "for": "15m"
        "labels":
          "severity": "critical"
  - data: |-
      {
        "alert": {
          "condition": "sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum{})\n  /\nsum(kube_node_status_allocatable_cpu_cores)\n  >\n(count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeCPUOvercommit",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "5m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{})\n  /\nsum(kube_node_status_allocatable_memory_bytes)\n  >\n(count(kube_node_status_allocatable_memory_bytes)-1)\n  /\ncount(kube_node_status_allocatable_memory_bytes)\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeMemOvercommit",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "5m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=\"cpu\"})\n  /\nsum(kube_node_status_allocatable_cpu_cores)\n  > 1.5\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeCPUOvercommit",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "5m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\", resource=\"memory\"})\n  /\nsum(kube_node_status_allocatable_memory_bytes{job=\"node-exporter\"})\n  > 1.5\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeMemOvercommit",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "5m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "kube_resourcequota{job=\"kube-state-metrics\", type=\"used\"}\n  / ignoring(instance, job, type)\n(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\"} > 0)\n  > 0.90\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeQuotaExceeded",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "sum(increase(container_cpu_cfs_throttled_periods_total{container!=\"\", }[5m])) by (container, pod, namespace)\n  /\nsum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)\n  > ( 25 / 100 )\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "CPUThrottlingHigh",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "kubelet_volume_stats_available_bytes{job=\"kubelet\"}\n  /\nkubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n  < 0.03\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubePersistentVolumeUsageCritical",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "1m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "(\n  kubelet_volume_stats_available_bytes{job=\"kubelet\"}\n    /\n  kubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n) < 0.15\nand\npredict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\"}[6h], 4 * 24 * 3600) < 0\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubePersistentVolumeFullInFourDays",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "1h",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "kube_persistentvolume_status_phase{phase=~\"Failed|Pending\",job=\"kube-state-metrics\"} > 0\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubePersistentVolumeErrors",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "5m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~\"kube-dns|coredns\"},\"gitVersion\",\"$1\",\"gitVersion\",\"(v[0-9]*.[0-9]*.[0-9]*).*\"))) > 1\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeVersionMismatch",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "(sum(rate(rest_client_requests_total{code=~\"5..\"}[5m])) by (instance, job)\n  /\nsum(rate(rest_client_requests_total[5m])) by (instance, job))\n> 0.01\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeClientErrors",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "(\n  status_class_5xx:apiserver_request_total:ratio_rate1h{job=\"kube-apiserver\"} > (14.4*0.010000)\n  and\n  status_class_5xx:apiserver_request_total:ratio_rate5m{job=\"kube-apiserver\"} > (14.4*0.010000)\n)\nor\n(\n  status_class_5xx:apiserver_request_total:ratio_rate6h{job=\"kube-apiserver\"} > (6*0.010000)\n  and\n  status_class_5xx:apiserver_request_total:ratio_rate30m{job=\"kube-apiserver\"} > (6*0.010000)\n)\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "ErrorBudgetBurn",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": 0,
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "(\n  status_class_5xx:apiserver_request_total:ratio_rate1d{job=\"kube-apiserver\"} > (3*0.010000)\n  and\n  status_class_5xx:apiserver_request_total:ratio_rate2h{job=\"kube-apiserver\"} > (3*0.010000)\n)\nor\n(\n  status_class_5xx:apiserver_request_total:ratio_rate3d{job=\"kube-apiserver\"} > (0.010000)\n  and\n  status_class_5xx:apiserver_request_total:ratio_rate6h{job=\"kube-apiserver\"} > (0.010000)\n)\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "ErrorBudgetBurn",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": 0,
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "(\n  cluster:apiserver_request_duration_seconds:mean5m{job=\"kube-apiserver\"}\n  >\n  on (verb) group_left()\n  (\n    avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job=\"kube-apiserver\"} >= 0)\n    +\n    2*stddev by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job=\"kube-apiserver\"} >= 0)\n  )\n) > on (verb) group_left()\n1.2 * avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job=\"kube-apiserver\"} >= 0)\nand on (verb,resource)\ncluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job=\"kube-apiserver\",quantile=\"0.99\"}\n>\n1\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeAPILatencyHigh",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "5m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job=\"kube-apiserver\",quantile=\"0.99\"} > 4\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeAPILatencyHigh",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "sum(rate(apiserver_request_total{job=\"kube-apiserver\",code=~\"5..\"}[5m])) by (resource,subresource,verb)\n  /\nsum(rate(apiserver_request_total{job=\"kube-apiserver\"}[5m])) by (resource,subresource,verb) > 0.10\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeAPIErrorsHigh",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "sum(rate(apiserver_request_total{job=\"kube-apiserver\",code=~\"5..\"}[5m])) by (resource,subresource,verb)\n  /\nsum(rate(apiserver_request_total{job=\"kube-apiserver\"}[5m])) by (resource,subresource,verb) > 0.05\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeAPIErrorsHigh",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "10m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "apiserver_client_certificate_expiration_seconds_count{job=\"kube-apiserver\"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"kube-apiserver\"}[5m]))) < 604800\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeClientCertificateExpiration",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": 0,
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "apiserver_client_certificate_expiration_seconds_count{job=\"kube-apiserver\"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"kube-apiserver\"}[5m]))) < 86400\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeClientCertificateExpiration",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": 0,
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "sum by(name, namespace)(increase(aggregator_unavailable_apiservice_count[5m])) > 2\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "AggregatedAPIErrors",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": 0,
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "sum by(name, namespace)(sum_over_time(aggregator_unavailable_apiservice[5m])) > 0\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "AggregatedAPIDown",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "5m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "absent(up{job=\"kube-apiserver\"} == 1)\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeAPIDown",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "kube_node_status_condition{job=\"kube-state-metrics\",condition=\"Ready\",status=\"true\"} == 0\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeNodeNotReady",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "kube_node_spec_taint{job=\"kube-state-metrics\",key=\"node.kubernetes.io/unreachable\",effect=\"NoSchedule\"} == 1\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeNodeUnreachable",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "2m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "max(max(kubelet_running_pod_count{job=\"kubelet\"}) by(instance) * on(instance) group_left(node) kubelet_node_name{job=\"kubelet\"}) by(node) / max(kube_node_status_capacity_pods{job=\"kube-state-metrics\"}) by(node) > 0.95\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeletTooManyPods",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "sum(changes(kube_node_status_condition{status=\"true\",condition=\"Ready\"}[15m])) by (node) > 2\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeNodeReadinessFlapping",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile=\"0.99\"} >= 10\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeletPlegDurationHigh",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "5m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job=\"kubelet\"}[5m])) by (instance, le)) * on(instance) group_left(node) kubelet_node_name  > 60\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeletPodStartUpLatencyHigh",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "absent(up{job=\"kubelet\"} == 1)\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeletDown",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "absent(up{job=\"kube-scheduler\"} == 1)\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeSchedulerDown",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
  - data: |-
      {
        "alert": {
          "condition": "absent(up{job=\"kube-controller-manager\"} == 1)\n",
          "customNotification": {
            "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
            "useNewTemplate": false
          },
          "enabled": true,
          "name": "KubeControllerManagerDown",
          "rateOfChange": false,
          "reNotify": false,
          "reNotifyMinutes": "15m",
          "severity": 4,
          "severityLabel": "LOW",
          "severityLevel": null,
          "timespan": 600000000,
          "type": "PROMETHEUS"
        }
      }
    kind: Sysdig
