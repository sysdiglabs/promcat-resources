apiVersion: v1
kind: Alert
app: Kubernetes control plane
version: 1.0.0
appVersion:
- 1.18.0
configurations:
- kind: Prometheus
  data: |
    - "alert": "KubeCPUOvercommit"
      "annotations":
        "message": "Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit"
      "expr": |
        sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum{})
          /
        sum(kube_node_status_allocatable_cpu_cores:sysdig)
          >
        (count(kube_node_status_allocatable_cpu_cores:sysdig)-1) / count(kube_node_status_allocatable_cpu_cores:sysdig)
      "for": "5m"
      "labels":
        "severity": "warning"
    - "alert": "KubeMemOvercommit"
      "annotations":
        "message": "Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit"
      "expr": |
        sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{})
          /
        sum(kube_node_status_allocatable_memory_bytes:sysdig)
          >
        (count(kube_node_status_allocatable_memory_bytes:sysdig)-1)
          /
        count(kube_node_status_allocatable_memory_bytes:sysdig)
      "for": "5m"
      "labels":
        "severity": "warning"
    - "alert": "KubeCPUOvercommit"
      "annotations":
        "message": "Cluster has overcommitted CPU resource requests for Namespaces."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit"
      "expr": |
        sum(kube_resourcequota:sysdig{job="kubernetes-service-endpoints", type="hard", resource="cpu"})
          /
        sum(kube_node_status_allocatable_cpu_cores:sysdig)
          > 1.5
      "for": "5m"
      "labels":
        "severity": "warning"
    - "alert": "KubeMemOvercommit"
      "annotations":
        "message": "Cluster has overcommitted memory resource requests for Namespaces."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit"
      "expr": |
        sum(kube_resourcequota:sysdig{job="kubernetes-service-endpoints", type="hard", resource="memory"})
          /
        sum(kube_node_status_allocatable_memory_bytes:sysdig{job="node-exporter"})
          > 1.5
      "for": "5m"
      "labels":
        "severity": "warning"
    - "alert": "KubeQuotaExceeded"
      "annotations":
        "message": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded"
      "expr": |
        kube_resourcequota:sysdig{job="kubernetes-service-endpoints", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota:sysdig{job="kubernetes-service-endpoints", type="hard"} > 0)
          > 0.90
      "for": "15m"
      "labels":
        "severity": "warning"
    - "alert": "CPUThrottlingHigh"
      "annotations":
        "message": "{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cputhrottlinghigh"
      "expr": |
        sum(increase(container_cpu_cfs_throttled_periods_total:sysdig{container!="", }[5m])) by (container, pod, namespace)
          /
        sum(increase(container_cpu_cfs_periods_total:sysdig{}[5m])) by (container, pod, namespace)
          > ( 25 / 100 )
      "for": "15m"
      "labels":
        "severity": "warning"

    - "alert": "KubePersistentVolumeUsageCritical"
      "annotations":
        "message": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical"
      "expr": |
        kubelet_volume_stats_available_bytes:sysdig{job="kubelet"}
          /
        kubelet_volume_stats_capacity_bytes:sysdig{job="kubelet"}
          < 0.03
      "for": "1m"
      "labels":
        "severity": "critical"
    - "alert": "KubePersistentVolumeFullInFourDays"
      "annotations":
        "message": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays"
      "expr": |
        (
          kubelet_volume_stats_available_bytes:sysdig{job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes:sysdig{job="kubelet"}
        ) < 0.15
        and
        predict_linear(kubelet_volume_stats_available_bytes:sysdig{job="kubelet"}[6h], 4 * 24 * 3600) < 0
      "for": "1h"
      "labels":
        "severity": "critical"
    - "alert": "KubePersistentVolumeErrors"
      "annotations":
        "message": "The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeerrors"
      "expr": |
        kube_persistentvolume_status_phase:sysdig{phase=~"Failed|Pending",job="kubernetes-service-endpoints"} > 0
      "for": "5m"
      "labels":
        "severity": "critical"

    - "alert": "KubeVersionMismatch"
      "annotations":
        "message": "There are {{ $value }} different semantic versions of Kubernetes components running."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch"
      "expr": |
        count(count by (gitVersion) (label_replace(kubernetes_build_info:sysdig{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
      "for": "15m"
      "labels":
        "severity": "warning"
    - "alert": "KubeClientErrors"
      "annotations":
        "message": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'"
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors"
      "expr": |
        (rest_client_requests_total:5xx:sysdig
          /
        rest_client_requests_total:sysdig
        > 0.01
      "for": "15m"
      "labels":
        "severity": "warning"

    - "alert": "ErrorBudgetBurn"
      "annotations":
        "message": "High requests error budget burn for job=kube-apiserver (current value: {{ $value }})"
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn"
      "expr": |
        (
          status_class_5xx:apiserver_request_total:ratio_rate1h{job="kube-apiserver"} > (14.4*0.010000)
          and
          status_class_5xx:apiserver_request_total:ratio_rate5m{job="kube-apiserver"} > (14.4*0.010000)
        )
        or
        (
          status_class_5xx:apiserver_request_total:ratio_rate6h{job="kube-apiserver"} > (6*0.010000)
          and
          status_class_5xx:apiserver_request_total:ratio_rate30m{job="kube-apiserver"} > (6*0.010000)
        )
      "labels":
        "job": "kube-apiserver"
        "severity": "critical"
    - "alert": "ErrorBudgetBurn"
      "annotations":
        "message": "High requests error budget burn for job=kube-apiserver (current value: {{ $value }})"
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn"
      "expr": |
        (
          status_class_5xx:apiserver_request_total:ratio_rate1d{job="kube-apiserver"} > (3*0.010000)
          and
          status_class_5xx:apiserver_request_total:ratio_rate2h{job="kube-apiserver"} > (3*0.010000)
        )
        or
        (
          status_class_5xx:apiserver_request_total:ratio_rate3d{job="kube-apiserver"} > (0.010000)
          and
          status_class_5xx:apiserver_request_total:ratio_rate6h{job="kube-apiserver"} > (0.010000)
        )
      "labels":
        "job": "kube-apiserver"
        "severity": "warning"

    - "alert": "KubeAPILatencyHigh"
      "annotations":
        "message": "The API server has an abnormal latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh"
      "expr": |
        (
          cluster:apiserver_request_duration_seconds:mean5m{job="kube-apiserver"}
          >
          on (verb) group_left()
          (
            avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kube-apiserver"} >= 0)
            +
            2*stddev by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kube-apiserver"} >= 0)
          )
        ) > on (verb) group_left()
        1.2 * avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="kube-apiserver"} >= 0)
        and on (verb,resource)
        cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="kube-apiserver",quantile="0.99"}
        >
        1
      "for": "5m"
      "labels":
        "severity": "warning"
    - "alert": "KubeAPILatencyHigh"
      "annotations":
        "message": "The API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh"
      "expr": |
        cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="kube-apiserver",quantile="0.99"} > 4
      "for": "10m"
      "labels":
        "severity": "critical"
    - "alert": "KubeAPIErrorsHigh"
      "annotations":
        "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh"
      "expr": |
        sum(rate(apiserver_request_total:sysdig{job="kube-apiserver",code=~"5.."}[5m])) by (resource,subresource,verb)
          /
        sum(rate(apiserver_request_total:sysdig{job="kube-apiserver"}[5m])) by (resource,subresource,verb) > 0.10
      "for": "10m"
      "labels":
        "severity": "critical"
    - "alert": "KubeAPIErrorsHigh"
      "annotations":
        "message": "API server is returning errors for {{ $value | humanizePercentage }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh"
      "expr": |
        sum(rate(apiserver_request_total:sysdig{job="kube-apiserver",code=~"5.."}[5m])) by (resource,subresource,verb)
          /
        sum(rate(apiserver_request_total:sysdig{job="kube-apiserver"}[5m])) by (resource,subresource,verb) > 0.05
      "for": "10m"
      "labels":
        "severity": "warning"
    - "alert": "KubeClientCertificateExpiration"
      "annotations":
        "message": "A client certificate used to authenticate to the apiserver is expiring in less than 7.0 days."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration"
      "expr": |
        apiserver_client_certificate_expiration_seconds_count:sysdig{job="kube-apiserver"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket:sysdig{job="kube-apiserver"}[5m]))) < 604800
      "labels":
        "severity": "warning"
    - "alert": "KubeClientCertificateExpiration"
      "annotations":
        "message": "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration"
      "expr": |
        apiserver_client_certificate_expiration_seconds_count:sysdig{job="kube-apiserver"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket:sysdig{job="kube-apiserver"}[5m]))) < 86400
      "labels":
        "severity": "critical"
    - "alert": "AggregatedAPIErrors"
      "annotations":
        "message": "An aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. The number of errors have increased for it in the past five minutes. High values indicate that the availability of the service changes too often."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapierrors"
      "expr": |
        sum by(name, namespace)(increase(aggregator_unavailable_apiservice_count:sysdig[5m])) > 2
      "labels":
        "severity": "warning"
    - "alert": "AggregatedAPIDown"
      "annotations":
        "message": "An aggregated API {{ $labels.name }}/{{ $labels.namespace }} is down. It has not been available at least for the past five minutes."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-aggregatedapidown"
      "expr": |
        sum by(name, namespace)(sum_over_time(aggregator_unavailable_apiservice:sysdig[5m])) > 0
      "for": "5m"
      "labels":
        "severity": "warning"
    - "alert": "KubeAPIDown"
      "annotations":
        "message": "KubeAPI has disappeared from Prometheus target discovery."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown"
      "expr": |
        absent(up{job="kube-apiserver"} == 1)
      "for": "15m"
      "labels":
        "severity": "critical"

    - "alert": "KubeNodeNotReady"
      "annotations":
        "message": "{{ $labels.node }} has been unready for more than 15 minutes."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready"
      "expr": |
        kube_node_status_condition:sysdig{job="kubernetes-service-endpoints",condition="Ready",status="true"} == 0
      "for": "15m"
      "labels":
        "severity": "warning"
    - "alert": "KubeNodeUnreachable"
      "annotations":
        "message": "{{ $labels.node }} is unreachable and some workloads may be rescheduled."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodeunreachable"
      "expr": |
        kube_node_spec_taint:sysdig{job="kubernetes-service-endpoints",key="node.kubernetes.io/unreachable",effect="NoSchedule"} == 1
      "for": "2m"
      "labels":
        "severity": "warning"
    - "alert": "KubeletTooManyPods"
      "annotations":
        "message": "Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods"
      "expr": |
        max(max(kubelet_running_pod_count:sysdig{job="kubelet"}) by(instance) * on(instance) group_left(node) kubelet_node_name:sysdig{job="kubelet"}) by(node) / max(kube_node_status_capacity_pods{job="kubernetes-service-endpoints"}) by(node) > 0.95
      "for": "15m"
      "labels":
        "severity": "warning"
    - "alert": "KubeNodeReadinessFlapping"
      "annotations":
        "message": "The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodereadinessflapping"
      "expr": |
        sum(changes(kube_node_status_condition:sysdig{status="true",condition="Ready"}[15m])) by (node) > 2
      "for": "15m"
      "labels":
        "severity": "warning"
    - "alert": "KubeletPlegDurationHigh"
      "annotations":
        "message": "The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletplegdurationhigh"
      "expr": |
        node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
      "for": "5m"
      "labels":
        "severity": "warning"
    - "alert": "KubeletPodStartUpLatencyHigh"
      "annotations":
        "message": "Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletpodstartuplatencyhigh"
      "expr": |
        histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet"}[5m])) by (instance, le)) * on(instance) group_left(node) kubelet_node_name:sysdig  > 60
      "for": "15m"
      "labels":
        "severity": "warning"
    - "alert": "KubeletDown"
      "annotations":
        "message": "Kubelet has disappeared from Prometheus target discovery."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown"
      "expr": |
        absent(up{job="kubelet"} == 1)
      "for": "15m"
      "labels":
        "severity": "critical"

    - "alert": "KubeSchedulerDown"
      "annotations":
        "message": "KubeScheduler has disappeared from Prometheus target discovery."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown"
      "expr": |
        absent(up{job="kube-scheduler"} == 1)
      "for": "15m"
      "labels":
        "severity": "critical"

    - "alert": "KubeControllerManagerDown"
      "annotations":
        "message": "KubeControllerManager has disappeared from Prometheus target discovery."
        "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown"
      "expr": |
        absent(up{job="kube-controller-manager"} == 1)
      "for": "15m"
      "labels":
        "severity": "critical"

    - alert: CoreDNSDown
      annotations:
        message: CoreDNS has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednsdown
      expr: |
        absent(up{job="kube-dns"} == 1)
      for: 15m
      labels:
        severity: critical

    - alert: CoreDNSLatencyHigh
      annotations:
        message: CoreDNS has 99th percentile latency of {{ $value }} seconds for server
          {{ $labels.server }} zone {{ $labels.zone }} .
        runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednslatencyhigh
      expr: |
        histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket{job="kube-dns"}[5m])) by(server, zone, le)) > 4
      for: 10m
      labels:
        severity: critical

    - alert: CoreDNSErrorsHigh
      annotations:
        message: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of
          requests.
        runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednserrorshigh
      expr: |
        sum(rate(coredns_dns_response_rcode_count_total{job="kube-dns",rcode="SERVFAIL"}[5m]))
          /
        sum(rate(coredns_dns_response_rcode_count_total{job="kube-dns"}[5m])) > 0.03
      for: 10m
      labels:
        severity: critical

    - alert: CoreDNSErrorsHigh
      annotations:
        message: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of
          requests.
        runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednserrorshigh
      expr: |
        sum(rate(coredns_dns_response_rcode_count_total{job="kube-dns",rcode="SERVFAIL"}[5m]))
          /
        sum(rate(coredns_dns_response_rcode_count_total{job="kube-dns"}[5m])) > 0.01
      for: 10m
      labels:
        severity: warning

    - alert: CoreDNSForwardLatencyHigh
      annotations:
        message: CoreDNS has 99th percentile latency of {{ $value }} seconds forwarding
          requests to {{ $labels.to }}.
        runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednsforwardlatencyhigh
      expr: |
        histogram_quantile(0.99, sum(rate(coredns_forward_request_duration_seconds_bucket{job="kube-dns"}[5m])) by(to, le)) > 4
      for: 10m
      labels:
        severity: critical

    - alert: CoreDNSForwardErrorsHigh
      annotations:
        message: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of
          forward requests to {{ $labels.to }}.
        runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednsforwarderrorshigh
      expr: |
        sum(rate(coredns_forward_response_rcode_count_total{job="kube-dns",rcode="SERVFAIL"}[5m]))
          /
        sum(rate(coredns_forward_response_rcode_count_total{job="kube-dns"}[5m])) > 0.03
      for: 10m
      labels:
        severity: critical

    - alert: CoreDNSForwardErrorsHigh
      annotations:
        message: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of
          forward requests to {{ $labels.to }}.
        runbook_url: https://github.com/povilasv/coredns-mixin/tree/master/runbook.md#alert-name-corednsforwarderrorshigh
      expr: |
        sum(rate(coredns_dns_response_rcode_count_total{job="kube-dns",rcode="SERVFAIL"}[5m]))
          /
        sum(rate(coredns_dns_response_rcode_count_total{job="kube-dns"}[5m])) > 0.01
      for: 10m
      labels:
        severity: warning
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum{})\n  /\nsum(kube_node_status_allocatable_cpu_cores:sysdig)\n  >\n(count(kube_node_status_allocatable_cpu_cores:sysdig)-1) / count(kube_node_status_allocatable_cpu_cores:sysdig)\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeCPUOvercommit",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 5,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{})\n  /\nsum(kube_node_status_allocatable_memory_bytes:sysdig)\n  >\n(count(kube_node_status_allocatable_memory_bytes:sysdig)-1)\n  /\ncount(kube_node_status_allocatable_memory_bytes:sysdig)\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeMemOvercommit",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 5,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(kube_resourcequota:sysdig{job=\"kubernetes-service-endpoints\", type=\"hard\", resource=\"cpu\"})\n  /\nsum(kube_node_status_allocatable_cpu_cores:sysdig)\n  > 1.5\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeCPUOvercommit",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 5,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(kube_resourcequota:sysdig{job=\"kubernetes-service-endpoints\", type=\"hard\", resource=\"memory\"})\n  /\nsum(kube_node_status_allocatable_memory_bytes:sysdig{job=\"node-exporter\"})\n  > 1.5\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeMemOvercommit",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 5,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "kube_resourcequota:sysdig{job=\"kubernetes-service-endpoints\", type=\"used\"}\n  / ignoring(instance, job, type)\n(kube_resourcequota:sysdig{job=\"kubernetes-service-endpoints\", type=\"hard\"} > 0)\n  > 0.90\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeQuotaExceeded",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(increase(container_cpu_cfs_throttled_periods_total:sysdig{container!=\"\", }[5m])) by (container, pod, namespace)\n  /\nsum(increase(container_cpu_cfs_periods_total:sysdig{}[5m])) by (container, pod, namespace)\n  > ( 25 / 100 )\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "CPUThrottlingHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "kubelet_volume_stats_available_bytes:sysdig{job=\"kubelet\"}\n  /\nkubelet_volume_stats_capacity_bytes:sysdig{job=\"kubelet\"}\n  < 0.03\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubePersistentVolumeUsageCritical",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 1,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "(\n  kubelet_volume_stats_available_bytes:sysdig{job=\"kubelet\"}\n    /\n  kubelet_volume_stats_capacity_bytes:sysdig{job=\"kubelet\"}\n) < 0.15\nand\npredict_linear(kubelet_volume_stats_available_bytes:sysdig{job=\"kubelet\"}[6h], 4 * 24 * 3600) < 0\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubePersistentVolumeFullInFourDays",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 60,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "kube_persistentvolume_status_phase:sysdig{phase=~\"Failed|Pending\",job=\"kubernetes-service-endpoints\"} > 0\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubePersistentVolumeErrors",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 5,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "count(count by (gitVersion) (label_replace(kubernetes_build_info:sysdig{job!~\"kube-dns|coredns\"},\"gitVersion\",\"$1\",\"gitVersion\",\"(v[0-9]*.[0-9]*.[0-9]*).*\"))) > 1\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeVersionMismatch",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "(rest_client_requests_total:5xx:sysdig\n  /\nrest_client_requests_total:sysdig\n> 0.01\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeClientErrors",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "(\n  status_class_5xx:apiserver_request_total:ratio_rate1h{job=\"kube-apiserver\"} > (14.4*0.010000)\n  and\n  status_class_5xx:apiserver_request_total:ratio_rate5m{job=\"kube-apiserver\"} > (14.4*0.010000)\n)\nor\n(\n  status_class_5xx:apiserver_request_total:ratio_rate6h{job=\"kube-apiserver\"} > (6*0.010000)\n  and\n  status_class_5xx:apiserver_request_total:ratio_rate30m{job=\"kube-apiserver\"} > (6*0.010000)\n)\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "ErrorBudgetBurn",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 0,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "(\n  status_class_5xx:apiserver_request_total:ratio_rate1d{job=\"kube-apiserver\"} > (3*0.010000)\n  and\n  status_class_5xx:apiserver_request_total:ratio_rate2h{job=\"kube-apiserver\"} > (3*0.010000)\n)\nor\n(\n  status_class_5xx:apiserver_request_total:ratio_rate3d{job=\"kube-apiserver\"} > (0.010000)\n  and\n  status_class_5xx:apiserver_request_total:ratio_rate6h{job=\"kube-apiserver\"} > (0.010000)\n)\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "ErrorBudgetBurn",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 0,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "(\n  cluster:apiserver_request_duration_seconds:mean5m{job=\"kube-apiserver\"}\n  >\n  on (verb) group_left()\n  (\n    avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job=\"kube-apiserver\"} >= 0)\n    +\n    2*stddev by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job=\"kube-apiserver\"} >= 0)\n  )\n) > on (verb) group_left()\n1.2 * avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job=\"kube-apiserver\"} >= 0)\nand on (verb,resource)\ncluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job=\"kube-apiserver\",quantile=\"0.99\"}\n>\n1\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeAPILatencyHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 5,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job=\"kube-apiserver\",quantile=\"0.99\"} > 4\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeAPILatencyHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 10,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(rate(apiserver_request_total:sysdig{job=\"kube-apiserver\",code=~\"5..\"}[5m])) by (resource,subresource,verb)\n  /\nsum(rate(apiserver_request_total:sysdig{job=\"kube-apiserver\"}[5m])) by (resource,subresource,verb) > 0.10\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeAPIErrorsHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 10,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(rate(apiserver_request_total:sysdig{job=\"kube-apiserver\",code=~\"5..\"}[5m])) by (resource,subresource,verb)\n  /\nsum(rate(apiserver_request_total:sysdig{job=\"kube-apiserver\"}[5m])) by (resource,subresource,verb) > 0.05\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeAPIErrorsHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 10,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "apiserver_client_certificate_expiration_seconds_count:sysdig{job=\"kube-apiserver\"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket:sysdig{job=\"kube-apiserver\"}[5m]))) < 604800\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeClientCertificateExpiration",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 0,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "apiserver_client_certificate_expiration_seconds_count:sysdig{job=\"kube-apiserver\"} > 0 and on(job) histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket:sysdig{job=\"kube-apiserver\"}[5m]))) < 86400\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeClientCertificateExpiration",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 0,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum by(name, namespace)(increase(aggregator_unavailable_apiservice_count:sysdig[5m])) > 2\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "AggregatedAPIErrors",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 0,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum by(name, namespace)(sum_over_time(aggregator_unavailable_apiservice:sysdig[5m])) > 0\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "AggregatedAPIDown",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 5,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "absent(up{job=\"kube-apiserver\"} == 1)\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeAPIDown",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "kube_node_status_condition:sysdig{job=\"kubernetes-service-endpoints\",condition=\"Ready\",status=\"true\"} == 0\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeNodeNotReady",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "kube_node_spec_taint:sysdig{job=\"kubernetes-service-endpoints\",key=\"node.kubernetes.io/unreachable\",effect=\"NoSchedule\"} == 1\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeNodeUnreachable",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 2,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "max(max(kubelet_running_pod_count:sysdig{job=\"kubelet\"}) by(instance) * on(instance) group_left(node) kubelet_node_name:sysdig{job=\"kubelet\"}) by(node) / max(kube_node_status_capacity_pods{job=\"kubernetes-service-endpoints\"}) by(node) > 0.95\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeletTooManyPods",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(changes(kube_node_status_condition:sysdig{status=\"true\",condition=\"Ready\"}[15m])) by (node) > 2\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeNodeReadinessFlapping",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile=\"0.99\"} >= 10\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeletPlegDurationHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 5,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job=\"kubelet\"}[5m])) by (instance, le)) * on(instance) group_left(node) kubelet_node_name:sysdig  > 60\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeletPodStartUpLatencyHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "absent(up{job=\"kubelet\"} == 1)\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeletDown",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "absent(up{job=\"kube-scheduler\"} == 1)\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeSchedulerDown",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "absent(up{job=\"kube-controller-manager\"} == 1)\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "KubeControllerManagerDown",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "absent(up{job=\"kube-dns\"} == 1)\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "CoreDNSDown",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 15,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket{job=\"kube-dns\"}[5m])) by(server, zone, le)) > 4\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "CoreDNSLatencyHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 10,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(rate(coredns_dns_response_rcode_count_total{job=\"kube-dns\",rcode=\"SERVFAIL\"}[5m]))\n  /\nsum(rate(coredns_dns_response_rcode_count_total{job=\"kube-dns\"}[5m])) > 0.03\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "CoreDNSErrorsHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 10,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(rate(coredns_dns_response_rcode_count_total{job=\"kube-dns\",rcode=\"SERVFAIL\"}[5m]))\n  /\nsum(rate(coredns_dns_response_rcode_count_total{job=\"kube-dns\"}[5m])) > 0.01\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "CoreDNSErrorsHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 10,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "histogram_quantile(0.99, sum(rate(coredns_forward_request_duration_seconds_bucket{job=\"kube-dns\"}[5m])) by(to, le)) > 4\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "CoreDNSForwardLatencyHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 10,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(rate(coredns_forward_response_rcode_count_total{job=\"kube-dns\",rcode=\"SERVFAIL\"}[5m]))\n  /\nsum(rate(coredns_forward_response_rcode_count_total{job=\"kube-dns\"}[5m])) > 0.03\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "CoreDNSForwardErrorsHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 10,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
- kind: Sysdig
  data: |-
    {
      "alert": {
        "condition": "sum(rate(coredns_dns_response_rcode_count_total{job=\"kube-dns\",rcode=\"SERVFAIL\"}[5m]))\n  /\nsum(rate(coredns_dns_response_rcode_count_total{job=\"kube-dns\"}[5m])) > 0.01\n",
        "customNotification": {
          "titleTemplate": "{{__alert_name__}} is {{__alert_status__}}",
          "useNewTemplate": false
        },
        "enabled": true,
        "name": "CoreDNSForwardErrorsHigh",
        "rateOfChange": false,
        "reNotify": false,
        "reNotifyMinutes": 10,
        "severity": 4,
        "severityLabel": "LOW",
        "severityLevel": null,
        "timespan": 600000000,
        "type": "PROMETHEUS"
      }
    }
descriptionFile: ALERTSv1.18.md